OR <- exp(coef(m2))
OR
OR.CI <- exp(confint(m2))
OR.CI
# Confusion Matrix on Trainset
prob.train <- predict(m2, type = 'response')
m2.predict.train <- ifelse(prob.train > threshold1, "1", "0")
table3 <- table(Trainset.Actual =  trainset$HeartDisease, m2.predict.train, deparse.level = 2)
cfm3 <- as_tibble(table3)
# Confusion matrix
plot_confusion_matrix(cfm3,
target_col = "Trainset.Actual",
prediction_col = "m2.predict.train",
counts_col = "n",
place_x_axis_above = FALSE,
add_col_percentages = FALSE,
add_row_percentages = FALSE)
# Overall Accuracy
mean(m2.predict.train == trainset$HeartDisease)
# Confusion Matrix on Testset
prob.test <- predict(m2, newdata = testset, type = 'response')
m2.predict.test <- ifelse(prob.test > threshold1, "1", "0")
table4 <- table(Testset.Actual = testset$HeartDisease, m2.predict.test, deparse.level = 2)
cfm4 <- as_tibble(table4)
cfm4
plot_confusion_matrix(cfm4,
target_col = "Testset.Actual",
prediction_col = "m2.predict.test",
counts_col = "n",
place_x_axis_above = FALSE,
add_col_percentages = FALSE,
add_row_percentages = FALSE)
# Overall Accuracy
mean(m2.predict.test == testset$HeartDisease)
set.seed(2000)
train <- sample.split(Y = heart_failure_dt$HeartDisease, SplitRatio = 0.7 )
trainset <- subset(heart_failure_dt, train == T)
testset <- subset(heart_failure_dt, train == F)
m1 <- glm(HeartDisease ~ . , family = binomial, data = trainset)
summary(m1)
# Check Multicollinearity
# For categorical variable if GVIF < 2 there is no Multicollinearity
# Insight from VIF Function: All the GVIF & adjusted GVIF are less than 2, hence there is no issue of multicollinearity in our model.
vif(m1)
OR <- exp(coef(m1))
OR
OR.CI <- exp(confint(m1))
OR.CI
# Insight: A few variables are statistically insignificant from the p-values observed. Hence, we use the 95% confidence interval to check which
# variables to drop. From the confidence interval,some X variables which contain 1 in their confidence interval are RestingBP,RestingECG, MAXHR, ST_Slope.
# We combine this with the results from the step function and confirm that these 4 variables are statistically insignificant and thus remove them from our model.
threshold1 <- 0.5
# Confusion Matrix on Trainset
prob.train <- predict(m1, type = 'response')
m1.predict.train <- ifelse(prob.train > threshold1, "1", "0")
table1 <- table(Trainset.Actual =  trainset$HeartDisease, m1.predict.train, deparse.level = 2)
table1
cfm1 <- as_tibble(table1)
cfm1
# Confusion matrix
plot_confusion_matrix(cfm1,
target_col = "Trainset.Actual",
prediction_col = "m1.predict.train",
counts_col = "n",
place_x_axis_above = FALSE,
add_col_percentages = FALSE,
add_row_percentages = FALSE)
# Overall Accuracy
mean(m1.predict.train == trainset$HeartDisease)
# Confusion Matrix on Testset
prob.test <- predict(m1, newdata = testset, type = 'response')
m1.predict.test <- ifelse(prob.test > threshold1, "1", "0")
table2 <- table(Testset.Actual = testset$HeartDisease, m1.predict.test, deparse.level = 2)
table2
cfm2 <- as_tibble(table2)
cfm2
plot_confusion_matrix(cfm2,
target_col = "Testset.Actual",
prediction_col = "m1.predict.test",
counts_col = "n",
place_x_axis_above = FALSE,
add_col_percentages = FALSE,
add_row_percentages = FALSE)
# Overall Accuracy
mean(m1.predict.test == testset$HeartDisease)
m2 <- glm(HeartDisease ~ . -Age -RestingECG -RestingBP -MaxHR -Cholesterol, family = binomial, data = trainset)
summary(m2)
# Check Multicollinearity
# For categorical variable if GVIF < 2 there is no Multicollinearity
# Insight from VIF Function: All the GVIF & adjusted GVIF are less than 2, hence there is no issue of multicollinearity in our model.
vif(m2)
OR <- exp(coef(m2))
OR
OR.CI <- exp(confint(m2))
OR.CI
# Confusion Matrix on Trainset
prob.train <- predict(m2, type = 'response')
m2.predict.train <- ifelse(prob.train > threshold1, "1", "0")
table3 <- table(Trainset.Actual =  trainset$HeartDisease, m2.predict.train, deparse.level = 2)
cfm3 <- as_tibble(table3)
# Confusion matrix
plot_confusion_matrix(cfm3,
target_col = "Trainset.Actual",
prediction_col = "m2.predict.train",
counts_col = "n",
place_x_axis_above = FALSE,
add_col_percentages = FALSE,
add_row_percentages = FALSE)
# Overall Accuracy
mean(m2.predict.train == trainset$HeartDisease)
# Confusion Matrix on Testset
prob.test <- predict(m2, newdata = testset, type = 'response')
m2.predict.test <- ifelse(prob.test > threshold1, "1", "0")
table4 <- table(Testset.Actual = testset$HeartDisease, m2.predict.test, deparse.level = 2)
cfm4 <- as_tibble(table4)
cfm4
plot_confusion_matrix(cfm4,
target_col = "Testset.Actual",
prediction_col = "m2.predict.test",
counts_col = "n",
place_x_axis_above = FALSE,
add_col_percentages = FALSE,
add_row_percentages = FALSE)
# Overall Accuracy
mean(m2.predict.test == testset$HeartDisease)
set.seed(1711)
train <- sample.split(Y = heart_failure_dt$HeartDisease, SplitRatio = 0.7 )
trainset <- subset(heart_failure_dt, train == T)
testset <- subset(heart_failure_dt, train == F)
str(trainset)
#---------------------------------------------Testing with all variables------------------------------------------------------#
m1 <- glm(HeartDisease ~ . , family = binomial, data = trainset)
summary(m1)
# Check Multicollinearity
# For categorical variable if GVIF < 2 there is no Multicollinearity
# Insight from VIF Function: All the GVIF & adjusted GVIF are less than 2, hence there is no issue of multicollinearity in our model.
vif(m1)
OR <- exp(coef(m1))
OR
OR.CI <- exp(confint(m1))
OR.CI
# Insight: A few variables are statistically insignificant from the p-values observed. Hence, we use the 95% confidence interval to check which
# variables to drop. From the confidence interval,some X variables which contain 1 in their confidence interval are RestingBP,RestingECG, MAXHR, ST_Slope.
# We combine this with the results from the step function and confirm that these 4 variables are statistically insignificant and thus remove them from our model.
threshold1 <- 0.5
# Confusion Matrix on Trainset
prob.train <- predict(m1, type = 'response')
m1.predict.train <- ifelse(prob.train > threshold1, "1", "0")
table1 <- table(Trainset.Actual =  trainset$HeartDisease, m1.predict.train, deparse.level = 2)
table1
cfm1 <- as_tibble(table1)
cfm1
# Confusion matrix
plot_confusion_matrix(cfm1,
target_col = "Trainset.Actual",
prediction_col = "m1.predict.train",
counts_col = "n",
place_x_axis_above = FALSE,
add_col_percentages = FALSE,
add_row_percentages = FALSE)
# Overall Accuracy
mean(m1.predict.train == trainset$HeartDisease)
# Confusion Matrix on Testset
prob.test <- predict(m1, newdata = testset, type = 'response')
m1.predict.test <- ifelse(prob.test > threshold1, "1", "0")
table2 <- table(Testset.Actual = testset$HeartDisease, m1.predict.test, deparse.level = 2)
table2
cfm2 <- as_tibble(table2)
cfm2
plot_confusion_matrix(cfm2,
target_col = "Testset.Actual",
prediction_col = "m1.predict.test",
counts_col = "n",
place_x_axis_above = FALSE,
add_col_percentages = FALSE,
add_row_percentages = FALSE)
# Overall Accuracy
mean(m1.predict.test == testset$HeartDisease)
# Accuracy of about 85%
#---------------------------------------------Testing with significant variables------------------------------------------------------#
m2 <- glm(HeartDisease ~ . -Age -RestingECG -RestingBP -MaxHR -Cholesterol, family = binomial, data = trainset)
summary(m2)
# Check Multicollinearity
# For categorical variable if GVIF < 2 there is no Multicollinearity
# Insight from VIF Function: All the GVIF & adjusted GVIF are less than 2, hence there is no issue of multicollinearity in our model.
vif(m2)
OR <- exp(coef(m2))
OR
OR.CI <- exp(confint(m2))
OR.CI
# Confusion Matrix on Trainset
prob.train <- predict(m2, type = 'response')
m2.predict.train <- ifelse(prob.train > threshold1, "1", "0")
table3 <- table(Trainset.Actual =  trainset$HeartDisease, m2.predict.train, deparse.level = 2)
cfm3 <- as_tibble(table3)
# Confusion matrix
plot_confusion_matrix(cfm3,
target_col = "Trainset.Actual",
prediction_col = "m2.predict.train",
counts_col = "n",
place_x_axis_above = FALSE,
add_col_percentages = FALSE,
add_row_percentages = FALSE)
# Overall Accuracy
mean(m2.predict.train == trainset$HeartDisease)
# Confusion Matrix on Testset
prob.test <- predict(m2, newdata = testset, type = 'response')
m2.predict.test <- ifelse(prob.test > threshold1, "1", "0")
table4 <- table(Testset.Actual = testset$HeartDisease, m2.predict.test, deparse.level = 2)
cfm4 <- as_tibble(table4)
cfm4
plot_confusion_matrix(cfm4,
target_col = "Testset.Actual",
prediction_col = "m2.predict.test",
counts_col = "n",
place_x_axis_above = FALSE,
add_col_percentages = FALSE,
add_row_percentages = FALSE)
# Overall Accuracy
mean(m2.predict.test == testset$HeartDisease)
write.csv(trainset, "heart_failure_prediction_training.csv", row.names = FALSE)
write.csv(testset, "heart_failure_prediction_test.csv", row.names = FALSE)
saveRDS(m2, "model.rds")
#---------------------------------------------Setting Working Directory------------------------------------------------------#
setwd("C:/Users/Wei Kang/OneDrive - Nanyang Technological University/Year 2/Sem 1/BC2406 Analytics I/BC2406 Course Materials/Group Project/Datasets/Heart Failure Prediction")
library(data.table)
library(rpart)
library(rpart.plot)
library(tidyverse)
library(caTools)
heart_failure_dt <-fread("heart_failure_cleaned.csv")
heart_failure_dt %>% glimpse()
heart_failure_dt %>% head()
heart_failure_dt %>% summary()
#---------------------------------------------Factorising Categorical Variables----------------------------------------------#
heart_failure_dt <- heart_failure_dt %>%
mutate_if(is.character, as.factor)
heart_failure_dt$FastingBS <- factor(heart_failure_dt$FastingBS)
heart_failure_dt$HeartDisease <- factor(heart_failure_dt$HeartDisease)
heart_failure_dt %>% summary()
set.seed(1711)
train <- sample.split(Y = heart_failure_dt$HeartDisease, SplitRatio = 0.7)
trainset <- subset(heart_failure_dt, train == T)
testset <- subset(heart_failure_dt, train == F)
m1 <- rpart(HeartDisease ~ ., data = trainset, method = 'class',
control = rpart.control(minsplit = 2, cp = 0))
rpart.plot(m1, nn= T, main = "Maximal Tree in heart_failure_dt.csv")
# prints the maximal tree m1 onto the console.
print(m1)
# prints out the pruning sequence and 10-fold CV errors, as a table.
printcp(m1)
# Display the pruning sequence and 10-fold CV errors, as a chart.
plotcp(m1, main = "Subtrees in heart_failure_dt.csv")
# Extract the Optimal Tree via code instead of eye power from the PlotCP
# Compute min CVerror + 1SE in maximal tree cart1.
CVerror.cap <- m1$cptable[which.min(m1$cptable[,"xerror"]), "xerror"] + m1$cptable[which.min(m1$cptable[,"xerror"]), "xstd"]
# Find the optimal CP region whose CV error is just below CVerror.cap in maximal tree.
i <- 1; j<- 4
while (m1$cptable[i,j] > CVerror.cap) {
i <- i + 1
}
# Get geometric mean of the two identified CP values in the optimal region if optimal tree has at least one split.
cp.opt = ifelse(i > 1, sqrt(m1$cptable[i,1] * m1$cptable[i-1,1]), 1)
#------Step 1.2: Pruning tree to the min-------#
m2 <-prune(m1,cp = cp.opt)
cp.opt
printcp(m2, digits = 3)
print(m2)
rpart.plot(m2,nn= T, main = "Pruned Tree with cp = 0.01776662" )
prp(m2, box.palette = "GnYlRd", main = "Pruned Tree with cp = 0.01776662")
#------Step 1.3: Making Predictions------#
cart.predict <- predict(m2, newdata = testset, type = "class")
table2 <- table(Testset.Actual = testset$HeartDisease, cart.predict, deparse.level = 2)
table2
round(prop.table(table2), 3)
# Overall Accuracy
mean(cart.predict == testset$HeartDisease)
# INSIGHT: Accuracy relatively low at about 86%. Can we do better?
# Finding the most important variables
m2$variable.importance
m3 <- rpart(HeartDisease ~ . -Age -RestingECG -RestingBP -MaxHR -Cholesterol, data = trainset, method = 'class',
control = rpart.control(minsplit = 2, cp = 0))
# plots the maximal tree and results. As we can observe, the tree contains too many nodes and is hence impossible to read. Some pruning is required later.
rpart.plot(m3, nn= T, main = "Maximal Tree in heart_failure_dt.csv")
# prints the maximal tree m1 onto the console.
print(m3)
# prints out the pruning sequence and 10-fold CV errors, as a table.
printcp(m3)
# Display the pruning sequence and 10-fold CV errors, as a chart.
plotcp(m3, main = "Subtrees in heart_failure_dt.csv")
CVerror.cap <- m3$cptable[which.min(m3$cptable[,"xerror"]), "xerror"] + m3$cptable[which.min(m3$cptable[,"xerror"]), "xstd"]
# Find the optimal CP region whose CV error is just below CVerror.cap in maximal tree.
i <- 1; j<- 4
while (m3$cptable[i,j] > CVerror.cap) {
i <- i + 1
}
# Get geometric mean of the two identified CP values in the optimal region if optimal tree has at least one split.
cp.opt = ifelse(i > 1, sqrt(m3$cptable[i,1] * m3$cptable[i-1,1]), 1)
m4 <-prune(m3,cp = cp.opt)
cp.opt
printcp(m4, digits = 3)
print(m4)
rpart.plot(m4,nn= T, main = "Pruned Tree with cp = 0.01404575" )
prp(m4, box.palette = "GnYlRd", main = "Pruned Tree with cp = 0.01404575")
#------Step 2.3: Making Predictions------#
cart.predict2 <- predict(m4, newdata = testset, type = "class")
table3 <- table(Testset.Actual = testset$HeartDisease, cart.predict2, deparse.level = 2)
table3
round(prop.table(table3), 3)
# Overall Accuracy
mean(cart.predict2 == testset$HeartDisease)
library(data.table)
library(cvms)
library(caTools)
library(tibble)
library(tidyverse)
#---------------------------------------------Setting Working Directory------------------------------------------------------#
setwd("C:/Users/Wei Kang/OneDrive - Nanyang Technological University/Year 2/Sem 1/BC2406 Analytics I/BC2406 Course Materials/Group Project/Datasets/BRFSS")
#-----------------------------------Importing Dataset & Getting an overview of the dataset-----------------------------------#
brfss_dt <-fread("BRFSS_2015_cleaned.csv")
brfss_dt %>% glimpse()
brfss_dt %>% head()
brfss_dt %>% summary()
rfss_dt$HeartDiseaseorAttack <- factor(brfss_dt$HeartDiseaseorAttack)
brfss_dt$HeartDiseaseorAttack <- factor(brfss_dt$HeartDiseaseorAttack)
brfss_dt$HighBP <- factor(brfss_dt$HighBP)
brfss_dt$HighChol <- factor(brfss_dt$HighChol)
brfss_dt$BMI <- factor(brfss_dt$BMI)
brfss_dt$Smoker <- factor(brfss_dt$Smoker)
brfss_dt$Stroke <- factor(brfss_dt$Stroke)
brfss_dt$Diabetes <- factor(brfss_dt$Diabetes)
brfss_dt$GenHlth <- factor(brfss_dt$GenHlth)
brfss_dt$MentHlth <- factor(brfss_dt$MentHlth)
brfss_dt$PhysHlth <- factor(brfss_dt$PhysHlth)
brfss_dt$Sex <- factor(brfss_dt$Sex)
brfss_dt$Age <- factor(brfss_dt$Age)
brfss_dt$Income <- factor(brfss_dt$Income)
brfss_dt %>% summary()
brfss_dt <-fread("BRFSS_2015_cleaned.csv")
brfss_dt %>% glimpse()
brfss_dt %>% head()
brfss_dt %>% summary()
#---------------------------------------------Factorising Categorical Variables----------------------------------------------#
brfss_dt$HeartDiseaseorAttack <- factor(brfss_dt$HeartDiseaseorAttack)
brfss_dt$HighBP <- factor(brfss_dt$HighBP)
brfss_dt$HighChol <- factor(brfss_dt$HighChol)
brfss_dt$Smoker <- factor(brfss_dt$Smoker)
brfss_dt$Stroke <- factor(brfss_dt$Stroke)
brfss_dt$Diabetes <- factor(brfss_dt$Diabetes)
brfss_dt$GenHlth <- factor(brfss_dt$GenHlth)
brfss_dt$MentHlth <- factor(brfss_dt$MentHlth)
brfss_dt$PhysHlth <- factor(brfss_dt$PhysHlth)
brfss_dt$Sex <- factor(brfss_dt$Sex)
brfss_dt$Age <- factor(brfss_dt$Age)
brfss_dt$Income <- factor(brfss_dt$Income)
brfss_dt %>% summary()
set.seed(1711)
train <- sample.split(Y = brfss_dt$HeartDiseaseorAttack, SplitRatio = 0.7)
trainset <- subset(brfss_dt, train == T)
testset <- subset(brfss_dt, train == F)
m1 <- glm(HeartDiseaseorAttack ~ . , family = binomial, data = trainset)
summary(m1)
threshold1 <- 0.5
# Confusion Matrix on Trainset
prob.train <- predict(m1, type = 'response')
m1.predict.train <- ifelse(prob.train > threshold1, "1", "0")
table1 <- table(Trainset.Actual =  trainset$HeartDiseaseorAttack, m1.predict.train, deparse.level = 2)
table1
cfm1 <- as_tibble(table1)
cfm1
# Confusion matrix
plot_confusion_matrix(cfm1,
target_col = "Trainset.Actual",
prediction_col = "m1.predict.train",
counts_col = "n",
place_x_axis_above = FALSE,
add_col_percentages = FALSE,
add_row_percentages = FALSE)
# Overall Accuracy
mean(m1.predict.train == trainset$HeartDiseaseorAttack)
# Confusion Matrix on Testset
prob.test <- predict(m1, newdata = testset, type = 'response')
m1.predict.test <- ifelse(prob.test > threshold1, "1", "0")
table2 <- table(Testset.Actual = testset$HeartDiseaseorAttack, m1.predict.test, deparse.level = 2)
table2
cfm2 <- as_tibble(table2)
cfm2
plot_confusion_matrix(cfm2,
target_col = "Testset.Actual",
prediction_col = "m1.predict.test",
counts_col = "n",
place_x_axis_above = FALSE,
add_col_percentages = FALSE,
add_row_percentages = FALSE)
# Overall Accuracy
mean(m1.predict.test == testset$HeartDiseaseorAttack)
# Check Multicollinearity
# For categorical variable if GVIF < 2 there is no Multicollinearity
# Insight from VIF Function: All the GVIF & adjusted GVIF are less than 2, hence there is no issue of multicollinearity in our model.
vif(m1)
library(data.table)
library(rpart)
library(rpart.plot)
library(tidyverse)
library(caTools)
brfss_dt <-fread("BRFSS_2015_cleaned.csv")
brfss_dt %>% glimpse()
brfss_dt %>% head()
brfss_dt %>% summary()
#---------------------------------------------Factorising Categorical Variables----------------------------------------------#
brfss_dt$HeartDiseaseorAttack <- factor(brfss_dt$HeartDiseaseorAttack)
brfss_dt$HighBP <- factor(brfss_dt$HighBP)
brfss_dt$HighChol <- factor(brfss_dt$HighChol)
brfss_dt$Smoker <- factor(brfss_dt$Smoker)
brfss_dt$Stroke <- factor(brfss_dt$Stroke)
brfss_dt$Diabetes <- factor(brfss_dt$Diabetes)
brfss_dt$GenHlth <- factor(brfss_dt$GenHlth)
brfss_dt$MentHlth <- factor(brfss_dt$MentHlth)
brfss_dt$PhysHlth <- factor(brfss_dt$PhysHlth)
brfss_dt$Sex <- factor(brfss_dt$Sex)
brfss_dt$Age <- factor(brfss_dt$Age)
brfss_dt$Income <- factor(brfss_dt$Income)
brfss_dt %>% summary()
set.seed(1711)
train <- sample.split(Y = brfss_dt$HeartDiseaseorAttack, SplitRatio = 0.7)
trainset <- subset(brfss_dt, train == T)
testset <- subset(brfss_dt, train == F)
m1 <- rpart(HeartDiseaseorAttack ~ ., data = trainset, method = 'class',
control = rpart.control(minsplit = 2, cp = 0))
# plots the maximal tree and results. As we can observe, the tree contains too many nodes and is hence impossible to read. Some pruning is required later.
rpart.plot(m1, nn= T, main = "Maximal Tree in heart_failure_dt.csv")
CVerror.cap <- m1$cptable[which.min(m1$cptable[,"xerror"]), "xerror"] + m1$cptable[which.min(m1$cptable[,"xerror"]), "xstd"]
# Find the optimal CP region whose CV error is just below CVerror.cap in maximal tree.
i <- 1; j<- 4
while (m1$cptable[i,j] > CVerror.cap) {
i <- i + 1
}
# Get geometric mean of the two identified CP values in the optimal region if optimal tree has at least one split.
cp.opt = ifelse(i > 1, sqrt(m1$cptable[i,1] * m1$cptable[i-1,1]), 1)
m2 <-prune(m1,cp = cp.opt)
cp.opt
printcp(m2, digits = 3)
print(m2)
rpart.plot(m2,nn= T, main = "Pruned Tree with cp = 0.001927732" )
prp(m2, box.palette = "GnYlRd", main = "Pruned Tree with cp = 0.001927732")
#------Step 1.3: Making Predictions------#
cart.predict <- predict(m2, newdata = testset, type = "class")
table2 <- table(Testset.Actual = testset$HeartDiseaseorAttack, cart.predict, deparse.level = 2)
table2
round(prop.table(table2), 3)
# Overall Accuracy
mean(cart.predict == testset$HeartDiseaseorAttack)
m2$variable.importance
set.seed(2000)
train <- sample.split(Y = brfss_dt$HeartDiseaseorAttack, SplitRatio = 0.7)
trainset <- subset(brfss_dt, train == T)
testset <- subset(brfss_dt, train == F)
#------------------------------------------------------CART Model------------------------------------------------------------#
#--------Step 1: Testing with all variables first-----#
#--------Step 1.1: Growing tree to the max-----#
m1 <- rpart(HeartDiseaseorAttack ~ ., data = trainset, method = 'class',
control = rpart.control(minsplit = 2, cp = 0))
CVerror.cap <- m1$cptable[which.min(m1$cptable[,"xerror"]), "xerror"] + m1$cptable[which.min(m1$cptable[,"xerror"]), "xstd"]
# Find the optimal CP region whose CV error is just below CVerror.cap in maximal tree.
i <- 1; j<- 4
while (m1$cptable[i,j] > CVerror.cap) {
i <- i + 1
}
# Get geometric mean of the two identified CP values in the optimal region if optimal tree has at least one split.
cp.opt = ifelse(i > 1, sqrt(m1$cptable[i,1] * m1$cptable[i-1,1]), 1)
#------Step 1.2: Pruning tree to the min-------#
m2 <-prune(m1,cp = cp.opt)
cp.opt
printcp(m2, digits = 3)
print(m2)
rpart.plot(m2,nn= T, main = "Pruned Tree with cp = 0.001927732" )
prp(m2, box.palette = "GnYlRd", main = "Pruned Tree with cp = 0.001927732")
#------Step 1.3: Making Predictions------#
cart.predict <- predict(m2, newdata = testset, type = "class")
table2 <- table(Testset.Actual = testset$HeartDiseaseorAttack, cart.predict, deparse.level = 2)
table2
round(prop.table(table2), 3)
# Overall Accuracy
mean(cart.predict == testset$HeartDiseaseorAttack)
set.seed(17)
train <- sample.split(Y = brfss_dt$HeartDiseaseorAttack, SplitRatio = 0.7)
trainset <- subset(brfss_dt, train == T)
testset <- subset(brfss_dt, train == F)
m1 <- rpart(HeartDiseaseorAttack ~ ., data = trainset, method = 'class',
control = rpart.control(minsplit = 2, cp = 0))
CVerror.cap <- m1$cptable[which.min(m1$cptable[,"xerror"]), "xerror"] + m1$cptable[which.min(m1$cptable[,"xerror"]), "xstd"]
# Find the optimal CP region whose CV error is just below CVerror.cap in maximal tree.
i <- 1; j<- 4
while (m1$cptable[i,j] > CVerror.cap) {
i <- i + 1
}
# Get geometric mean of the two identified CP values in the optimal region if optimal tree has at least one split.
cp.opt = ifelse(i > 1, sqrt(m1$cptable[i,1] * m1$cptable[i-1,1]), 1)
#------Step 1.2: Pruning tree to the min-------#
m2 <-prune(m1,cp = cp.opt)
cp.opt
printcp(m2, digits = 3)
print(m2)
rpart.plot(m2,nn= T, main = "Pruned Tree with cp = 0.001927732" )
prp(m2, box.palette = "GnYlRd", main = "Pruned Tree with cp = 0.001927732")
#------Step 1.3: Making Predictions------#
cart.predict <- predict(m2, newdata = testset, type = "class")
table2 <- table(Testset.Actual = testset$HeartDiseaseorAttack, cart.predict, deparse.level = 2)
table2
round(prop.table(table2), 3)
# Overall Accuracy
mean(cart.predict == testset$HeartDiseaseorAttack)
